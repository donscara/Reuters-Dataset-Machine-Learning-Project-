{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MACHINE LEARNING REUTERS DATASET ANALYSIS\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INTRODUCTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PROBLEM DEFINITION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the Universal Workflow blueprint to solve our machine learning problem.\n",
    "We use the blueprint illustrated in Deep Learning With Phyton by Francois Chollet.\n",
    "We will apply the following steps to make sure we are addressing the problem correctly.\n",
    "\n",
    "We are going first to define the problem we are facing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WHAT INPUT?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to use the Reuters Dataset which is a benchmark dataset for document classification.\n",
    "\n",
    "It consists of newswires and their topics which were published by Reutuers in 1986.\n",
    "\n",
    "The dataset is packaged inside Keras.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WHAT OUTPUT?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 46 different topics some are more common than others.\n",
    "\n",
    "We want to build a network to classify each newswire into 46 topics.\n",
    "\n",
    "The problem is one of multi-class classification therefore.\n",
    "\n",
    "We can also say that is a single-label multiclass classification since each newswire can only belong to a single topic.\n",
    "\n",
    "If each newswire could belong to multiple topics we would have a multi-label multiclass classification."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "import tensorflow\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.datasets import reuters\r\n",
    "# we split the dataset in train and test sets and restrict the data to the 10000 most frequent words\r\n",
    "(train_data,train_labels),(test_data,test_labels)=reuters.load_data(num_words=10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# we print the size of each set\r\n",
    "print(\"Train data length: \",len(train_data))\r\n",
    "print(\"Test data length: \",len(test_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data length:  8982\n",
      "Test data length:  2246\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# we can see the full list of indices and word associated\r\n",
    "tensorflow.keras.datasets.reuters.get_word_index(path=\"reuters_word_index.json\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mdbl': 10996,\n",
       " 'fawc': 16260,\n",
       " 'degussa': 12089,\n",
       " 'woods': 8803,\n",
       " 'hanging': 13796,\n",
       " 'localized': 20672,\n",
       " 'sation': 20673,\n",
       " 'chanthaburi': 20675,\n",
       " 'refunding': 10997,\n",
       " 'hermann': 8804,\n",
       " 'passsengers': 20676,\n",
       " 'stipulate': 20677,\n",
       " 'heublein': 8352,\n",
       " 'screaming': 20713,\n",
       " 'tcby': 16261,\n",
       " 'four': 185,\n",
       " 'grains': 1642,\n",
       " 'broiler': 20680,\n",
       " 'wooden': 12090,\n",
       " 'wednesday': 1220,\n",
       " 'highveld': 13797,\n",
       " 'duffour': 7593,\n",
       " '0053': 20681,\n",
       " 'elections': 3914,\n",
       " '270': 2563,\n",
       " '271': 3551,\n",
       " '272': 5113,\n",
       " '273': 3552,\n",
       " '274': 3400,\n",
       " 'rudman': 7975,\n",
       " '276': 3401,\n",
       " '277': 3478,\n",
       " '278': 3632,\n",
       " '279': 4309,\n",
       " 'dormancy': 9381,\n",
       " 'errors': 7247,\n",
       " 'deferred': 3086,\n",
       " 'sptnd': 20683,\n",
       " 'cooking': 8805,\n",
       " 'stratabit': 20684,\n",
       " 'designing': 16262,\n",
       " 'metalurgicos': 20685,\n",
       " 'databank': 13798,\n",
       " '300er': 20686,\n",
       " 'shocks': 20687,\n",
       " 'nawg': 7972,\n",
       " 'tnta': 20688,\n",
       " 'perforations': 20689,\n",
       " 'affiliates': 2891,\n",
       " '27p': 20690,\n",
       " 'ching': 16263,\n",
       " 'china': 595,\n",
       " 'wagyu': 16264,\n",
       " 'affiliated': 3189,\n",
       " 'chino': 16265,\n",
       " 'chinh': 16266,\n",
       " 'slickline': 20692,\n",
       " 'doldrums': 13799,\n",
       " 'kids': 12092,\n",
       " 'climbed': 3028,\n",
       " 'controversy': 6693,\n",
       " 'kidd': 20693,\n",
       " 'spotty': 12093,\n",
       " 'rebel': 12639,\n",
       " 'millimetres': 9382,\n",
       " 'golden': 4007,\n",
       " 'projection': 5689,\n",
       " 'stern': 12094,\n",
       " \"hudson's\": 7903,\n",
       " 'dna': 10066,\n",
       " 'dnc': 20695,\n",
       " 'hodler': 20696,\n",
       " 'lme': 2394,\n",
       " 'insolvancy': 20697,\n",
       " 'music': 13800,\n",
       " 'therefore': 1984,\n",
       " 'dns': 10998,\n",
       " 'distortions': 6959,\n",
       " 'thassos': 13801,\n",
       " 'populations': 20698,\n",
       " 'meteorologist': 8806,\n",
       " 'loss': 43,\n",
       " 'exco': 9383,\n",
       " 'adventist': 20813,\n",
       " 'murchison': 16267,\n",
       " 'locked': 10999,\n",
       " 'kampala': 13802,\n",
       " 'arndt': 20699,\n",
       " 'nakasone': 1267,\n",
       " 'steinweg': 20700,\n",
       " \"india's\": 3633,\n",
       " 'wang': 3029,\n",
       " 'wane': 10067,\n",
       " 'unjust': 13803,\n",
       " 'titanium': 13804,\n",
       " 'want': 850,\n",
       " 'pinto': 20701,\n",
       " \"institutes'\": 16268,\n",
       " 'absolute': 7973,\n",
       " 'travel': 4677,\n",
       " 'cutback': 6422,\n",
       " 'nazmi': 16269,\n",
       " 'modest': 1858,\n",
       " 'shopwell': 16270,\n",
       " 'sedi': 20702,\n",
       " 'adoped': 20703,\n",
       " 'tulis': 16271,\n",
       " '18th': 20704,\n",
       " \"wmc's\": 20705,\n",
       " 'menlo': 20706,\n",
       " 'reiners': 11000,\n",
       " 'farmlands': 12095,\n",
       " 'nonsensical': 20707,\n",
       " 'elisra': 20708,\n",
       " 'welcomed': 2461,\n",
       " 'peup': 20709,\n",
       " \"holiday's\": 16272,\n",
       " 'activating': 20711,\n",
       " 'avondale': 16273,\n",
       " 'interational': 16274,\n",
       " 'welcomes': 20712,\n",
       " 'fip': 16275,\n",
       " 'tailings': 11001,\n",
       " 'fit': 4205,\n",
       " 'lifeline': 16276,\n",
       " 'bringing': 1916,\n",
       " 'fix': 4819,\n",
       " '624': 6164,\n",
       " 'naturalite': 12096,\n",
       " 'wales': 6165,\n",
       " 'fin': 8807,\n",
       " 'fio': 11129,\n",
       " 'ceremenony': 20714,\n",
       " 'sovr': 20715,\n",
       " \"yeo's\": 20716,\n",
       " 'effects': 1788,\n",
       " 'sixteen': 13805,\n",
       " 'undeveloped': 8808,\n",
       " 'glutted': 13806,\n",
       " 'barton': 20717,\n",
       " 'froday': 20718,\n",
       " 'arrow': 10089,\n",
       " 'stabilises': 11002,\n",
       " 'allan': 6960,\n",
       " '374p': 20719,\n",
       " '393': 3891,\n",
       " '392': 4008,\n",
       " '391': 4206,\n",
       " '390': 3079,\n",
       " '397': 4550,\n",
       " '396': 6166,\n",
       " '395': 6423,\n",
       " '394': 4207,\n",
       " '399': 6961,\n",
       " '398': 4208,\n",
       " 'stabilised': 7595,\n",
       " 'smelters': 5114,\n",
       " 'oprah': 20720,\n",
       " 'orginially': 20721,\n",
       " \"tvx's\": 20722,\n",
       " 'ponomarev': 16278,\n",
       " 'enviroment': 20723,\n",
       " \"reeves'\": 20724,\n",
       " 'mason': 8363,\n",
       " 'encourage': 1670,\n",
       " 'adapt': 7596,\n",
       " 'abbott': 12776,\n",
       " 'stamping': 13808,\n",
       " 'colquiri': 20726,\n",
       " 'ambrit': 11003,\n",
       " 'strata': 8353,\n",
       " 'corrects': 4821,\n",
       " 'sandra': 11922,\n",
       " 'estimate': 859,\n",
       " 'universally': 20727,\n",
       " 'chlorine': 20728,\n",
       " 'competes': 16279,\n",
       " 'leiner': 10068,\n",
       " 'ministries': 8809,\n",
       " 'disturbed': 8810,\n",
       " 'competed': 13809,\n",
       " 'juergen': 8811,\n",
       " 'kfw': 13810,\n",
       " 'turben': 11004,\n",
       " 'reintroduced': 9384,\n",
       " 'maladies': 20729,\n",
       " 'chevron': 4101,\n",
       " 'lazere': 16280,\n",
       " 'antilles': 8812,\n",
       " 'dti': 11907,\n",
       " 'specially': 9070,\n",
       " 'bilzerian': 4678,\n",
       " 'bakelite': 13811,\n",
       " 'renovated': 20730,\n",
       " 'service': 568,\n",
       " 'payless': 16281,\n",
       " 'spiegler': 20731,\n",
       " 'needed': 831,\n",
       " 'wigglesworth': 16282,\n",
       " 'master': 6962,\n",
       " 'antonson': 13812,\n",
       " 'genesis': 20732,\n",
       " 'vismara': 13813,\n",
       " 'organically': 20734,\n",
       " \"accords'\": 20735,\n",
       " 'task': 5940,\n",
       " 'positively': 7974,\n",
       " 'feasibility': 3479,\n",
       " 'ahmed': 6963,\n",
       " \"suralco's\": 13814,\n",
       " 'awacs': 20736,\n",
       " 'idly': 16283,\n",
       " 'regulator': 20737,\n",
       " 'pseudorabies': 12097,\n",
       " 'staubli': 16284,\n",
       " 'nzi': 8813,\n",
       " 'feeling': 5115,\n",
       " '275': 3127,\n",
       " '6819': 20738,\n",
       " 'gorman': 16285,\n",
       " 'sustaining': 8354,\n",
       " 'spectrum': 9385,\n",
       " 'consenting': 20739,\n",
       " 'recapitalized': 12098,\n",
       " 'sailed': 11562,\n",
       " 'dozen': 7597,\n",
       " 'affairs': 1985,\n",
       " 'courier': 2253,\n",
       " 'kremlin': 8355,\n",
       " 'shipments': 895,\n",
       " \"aquino's\": 16286,\n",
       " 'committing': 10070,\n",
       " 'sugarcane': 5293,\n",
       " 'diminishing': 9386,\n",
       " 'vexing': 16287,\n",
       " 'simplify': 11005,\n",
       " 'mouth': 6167,\n",
       " 'steinhardt': 7248,\n",
       " 'conceded': 8814,\n",
       " 'bradford': 9387,\n",
       " 'singer': 7976,\n",
       " '5602': 20740,\n",
       " \"1987's\": 13816,\n",
       " 'tech': 4950,\n",
       " 'teck': 6424,\n",
       " 'majv': 20741,\n",
       " 'saying': 666,\n",
       " 'dickey': 16477,\n",
       " 'sweetner': 20742,\n",
       " 'teresa': 21149,\n",
       " 'ulcer': 20743,\n",
       " 'cheaply': 13817,\n",
       " 'thai': 2361,\n",
       " 'orleans': 6964,\n",
       " 'excavator': 16290,\n",
       " 'rico': 6168,\n",
       " 'lube': 12099,\n",
       " 'rick': 13818,\n",
       " 'rich': 4679,\n",
       " 'kerna': 13819,\n",
       " 'rice': 950,\n",
       " 'rica': 4209,\n",
       " 'plate': 5503,\n",
       " 'platt': 16291,\n",
       " 'altogether': 8356,\n",
       " 'jaguar': 8815,\n",
       " 'dynair': 20744,\n",
       " 'patch': 8816,\n",
       " 'ldp': 2892,\n",
       " 'boarded': 13820,\n",
       " 'precluding': 16292,\n",
       " 'clarified': 11006,\n",
       " 'sensitivity': 16293,\n",
       " 'alternative': 1511,\n",
       " 'clarifies': 11007,\n",
       " 'lots': 5116,\n",
       " 'irs': 7598,\n",
       " 'irv': 20745,\n",
       " 'iri': 13821,\n",
       " 'ira': 13822,\n",
       " 'timber': 5690,\n",
       " 'ire': 20746,\n",
       " 'discipline': 5219,\n",
       " 'extend': 1937,\n",
       " 'nature': 3634,\n",
       " \"amb's\": 16295,\n",
       " 'dunhill': 16296,\n",
       " 'extent': 2142,\n",
       " 'restrcitions': 20747,\n",
       " 'heating': 2396,\n",
       " \"mannesmann's\": 11008,\n",
       " 'outsanding': 20748,\n",
       " 'multimillions': 20749,\n",
       " 'sarcinelli': 13824,\n",
       " 'southeastern': 6694,\n",
       " 'eradicate': 10071,\n",
       " 'libyan': 9388,\n",
       " 'foreclosing': 20750,\n",
       " 'maclaine': 12101,\n",
       " 'fra': 20751,\n",
       " 'union': 353,\n",
       " 'frn': 11009,\n",
       " 'much': 386,\n",
       " 'fry': 12102,\n",
       " 'mothball': 20752,\n",
       " 'chlorazepate': 10072,\n",
       " 'dxns': 12103,\n",
       " 'toyko': 19981,\n",
       " 'spit': 20753,\n",
       " '007050': 16297,\n",
       " 'freehold': 16298,\n",
       " 'davy': 13825,\n",
       " 'dave': 11010,\n",
       " 'spie': 12177,\n",
       " 'aguayo': 10117,\n",
       " 'wildcat': 12104,\n",
       " 'fecs': 10069,\n",
       " 'kennan': 20754,\n",
       " 'intal': 16299,\n",
       " 'contingencies': 9389,\n",
       " 'professionally': 16551,\n",
       " 'microbiological': 16300,\n",
       " 'misconstrued': 20756,\n",
       " 'k': 409,\n",
       " 'securitiesd': 20757,\n",
       " 'deferring': 16301,\n",
       " 'kohl': 5941,\n",
       " 'conditioned': 3030,\n",
       " 'fnhb': 20758,\n",
       " \"october's\": 16302,\n",
       " 'memorial': 13954,\n",
       " 'democracies': 6965,\n",
       " 'conformed': 27520,\n",
       " 'split': 464,\n",
       " \"bond's\": 12105,\n",
       " 'thinly': 11112,\n",
       " 'dunkirk': 16515,\n",
       " 'cavanaugh': 16303,\n",
       " \"securities'\": 13827,\n",
       " 'marches': 21345,\n",
       " 'issam': 16304,\n",
       " 'workforce': 2020,\n",
       " 'meinert': 12106,\n",
       " 'boiler': 13828,\n",
       " \"bp's\": 5294,\n",
       " 'torpedoed': 16305,\n",
       " 'indidate': 20762,\n",
       " 'downwardly': 13829,\n",
       " 'viviez': 20763,\n",
       " 'vladiminovich': 20764,\n",
       " 'academic': 16306,\n",
       " 'architecural': 20765,\n",
       " 'corporate': 1117,\n",
       " 'appropriately': 16307,\n",
       " 'teicc': 20766,\n",
       " \"hanover's\": 20767,\n",
       " 'aristech': 8817,\n",
       " 'portrayed': 20768,\n",
       " 'raffineries': 21383,\n",
       " 'hai': 20770,\n",
       " 'hal': 7599,\n",
       " 'ham': 13830,\n",
       " 'han': 10073,\n",
       " 'e15b': 20771,\n",
       " 'had': 61,\n",
       " 'hay': 20772,\n",
       " 'botchwey': 13831,\n",
       " 'haq': 10074,\n",
       " 'has': 37,\n",
       " 'hat': 13832,\n",
       " 'hav': 20773,\n",
       " 'fortin': 20774,\n",
       " 'municipal': 8818,\n",
       " 'osman': 20775,\n",
       " 'fsical': 20776,\n",
       " 'elders': 3480,\n",
       " 'survival': 12107,\n",
       " 'unequivocally': 16308,\n",
       " 'objective': 2519,\n",
       " 'indicative': 6695,\n",
       " 'shadow': 10075,\n",
       " 'riskiness': 21411,\n",
       " 'positiive': 20778,\n",
       " \"american's\": 10076,\n",
       " 'alick': 16309,\n",
       " 'harima': 16310,\n",
       " 'alice': 12108,\n",
       " 'altschul': 20779,\n",
       " 'festivities': 16311,\n",
       " 'medecines': 20780,\n",
       " 'beneficial': 2942,\n",
       " 'yoweri': 12109,\n",
       " 'crowd': 13833,\n",
       " 'crowe': 9390,\n",
       " 'crown': 3553,\n",
       " 'topping': 13679,\n",
       " 'captive': 8819,\n",
       " 'billboard': 12110,\n",
       " 'fiduciary': 6169,\n",
       " 'bottom': 3402,\n",
       " 'plucked': 20782,\n",
       " 'locksmithing': 20783,\n",
       " 'ecopetrol': 9391,\n",
       " 'pipestone': 24018,\n",
       " \"growers'\": 5505,\n",
       " 'borrows': 20785,\n",
       " 'eduard': 16312,\n",
       " 'venpres': 13834,\n",
       " 'bamboo': 16313,\n",
       " 'foolish': 13835,\n",
       " 'uruguyan': 20786,\n",
       " 'officeholders': 20787,\n",
       " 'economiques': 20788,\n",
       " 'aden': 16314,\n",
       " 'maxwell': 4822,\n",
       " 'marshall': 4680,\n",
       " 'honeymoon': 16315,\n",
       " 'administer': 16316,\n",
       " 'shoots': 20790,\n",
       " 'rubbertech': 16317,\n",
       " 'johsen': 16318,\n",
       " 'reciprocity': 10077,\n",
       " 'fabric': 13836,\n",
       " 'suffice': 20791,\n",
       " 'spokemsan': 20792,\n",
       " \"sonora's\": 20793,\n",
       " '5865': 16319,\n",
       " \"systems'\": 16320,\n",
       " 'perfumes': 20794,\n",
       " 'halycon': 20795,\n",
       " 'nonvoting': 20796,\n",
       " 'safeguard': 7250,\n",
       " 'sawdust': 21538,\n",
       " \"else's\": 20797,\n",
       " 'arrays': 13837,\n",
       " 'aza': 20798,\n",
       " 'smasher': 20799,\n",
       " 'complications': 12111,\n",
       " 'pesos': 1813,\n",
       " 'relabelling': 20800,\n",
       " 'passenger': 3722,\n",
       " \"avon's\": 12112,\n",
       " 'megahertz': 20801,\n",
       " 'mirror': 10683,\n",
       " 'minas': 8357,\n",
       " 'bourdain': 16322,\n",
       " 'crownx': 20802,\n",
       " 'eventual': 6425,\n",
       " 'crowns': 1207,\n",
       " 'role': 1369,\n",
       " 'obliges': 20803,\n",
       " 'rolf': 16323,\n",
       " 'vegetative': 13838,\n",
       " 'rolm': 20804,\n",
       " 'roll': 4419,\n",
       " 'intend': 2463,\n",
       " 'palms': 16324,\n",
       " 'denys': 19255,\n",
       " 'transported': 13839,\n",
       " 'moresby': 20805,\n",
       " 'devon': 16325,\n",
       " 'intent': 1351,\n",
       " \"camco's\": 20806,\n",
       " 'variable': 5942,\n",
       " 'transporter': 20807,\n",
       " 'danske': 16326,\n",
       " 'friedhelm': 13840,\n",
       " 'hawker': 8358,\n",
       " \"sand's\": 17774,\n",
       " 'preseving': 20808,\n",
       " '80386': 12113,\n",
       " 'bnls': 16328,\n",
       " 'ordination': 19984,\n",
       " 'overturned': 11011,\n",
       " 'erred': 16329,\n",
       " 'cincinnati': 6696,\n",
       " 'corps': 16710,\n",
       " 'whoever': 20809,\n",
       " 'osp': 16330,\n",
       " 'osr': 13841,\n",
       " 'ost': 12114,\n",
       " 'chair': 16331,\n",
       " '690': 5647,\n",
       " 'grapples': 20810,\n",
       " 'megawatts': 13842,\n",
       " 'photocopiers': 20811,\n",
       " 'sconninx': 20812,\n",
       " 'circumstances': 2274,\n",
       " 'oversight': 13843,\n",
       " \"paradyne's\": 20814,\n",
       " '691': 6363,\n",
       " 'paychecks': 20815,\n",
       " \"stadelmann's\": 13844,\n",
       " 'choice': 3241,\n",
       " 'vastagh': 11012,\n",
       " 'embark': 8820,\n",
       " 'gloomy': 9392,\n",
       " 'stays': 9393,\n",
       " 'exact': 4009,\n",
       " 'minute': 5117,\n",
       " 'kittiwake': 11892,\n",
       " 'picul': 20816,\n",
       " 'skewed': 20817,\n",
       " 'cooke': 11013,\n",
       " 'defaults': 10078,\n",
       " 'reimpose': 11014,\n",
       " 'hindered': 9394,\n",
       " 'lengthened': 20818,\n",
       " 'chopping': 16333,\n",
       " 'mckiernan': 13845,\n",
       " 'collaspe': 20819,\n",
       " 'corazon': 7251,\n",
       " 'antwerp': 7600,\n",
       " 'abdullah': 13846,\n",
       " 'goldston': 13847,\n",
       " '300': 442,\n",
       " 'cassa': 20821,\n",
       " 'casse': 20822,\n",
       " '695': 4081,\n",
       " 'ground': 2979,\n",
       " 'boost': 839,\n",
       " 'azusa': 16334,\n",
       " 'drafted': 9395,\n",
       " '303': 4823,\n",
       " 'climbs': 13848,\n",
       " 'honour': 7601,\n",
       " 'vanderbilt': 20823,\n",
       " '305': 3968,\n",
       " 'address': 3031,\n",
       " 'dwindling': 8821,\n",
       " 'benson': 7252,\n",
       " 'enroll': 12115,\n",
       " 'revenues': 501,\n",
       " 'impacted': 12116,\n",
       " 'queue': 20826,\n",
       " 'accomplished': 10079,\n",
       " 'throughput': 7602,\n",
       " 'influx': 9396,\n",
       " 'stockbuilding': 10080,\n",
       " 'aproximates': 20827,\n",
       " 'petroleo': 13849,\n",
       " 'sistemas': 16335,\n",
       " 'feretti': 14053,\n",
       " 'opposes': 5943,\n",
       " 'working': 882,\n",
       " 'perished': 20829,\n",
       " 'oldham': 13850,\n",
       " '27000': 20830,\n",
       " 'optimize': 19245,\n",
       " 'vigour': 20832,\n",
       " 'opposed': 1580,\n",
       " 'liberalizing': 16336,\n",
       " 'wvz': 20833,\n",
       " 'dampness': 20834,\n",
       " 'approving': 13851,\n",
       " 'sierra': 13496,\n",
       " 'entrepot': 20835,\n",
       " 'currency': 224,\n",
       " 'originally': 1499,\n",
       " 'tindemans': 20837,\n",
       " 'valorem': 16337,\n",
       " 'following': 477,\n",
       " 'fossen': 20838,\n",
       " 'locke': 11016,\n",
       " 'employess': 20839,\n",
       " 'rotberg': 12117,\n",
       " 'parachute': 16338,\n",
       " 'locks': 11017,\n",
       " 'incremental': 12255,\n",
       " 'woolowrth': 16339,\n",
       " 'listens': 20841,\n",
       " 'litre': 7253,\n",
       " 'edouard': 3554,\n",
       " 'ounce': 1377,\n",
       " 'nicanor': 20843,\n",
       " 'sucocitrico': 20844,\n",
       " 'minicomputers': 16340,\n",
       " \"silva's\": 16341,\n",
       " 'restitutions': 11018,\n",
       " 'custer': 16342,\n",
       " '3rd': 2590,\n",
       " 'fueled': 10081,\n",
       " 'trydahl': 20845,\n",
       " 'aice': 11019,\n",
       " 'harmon': 12118,\n",
       " 'conscious': 10082,\n",
       " 'herbicidesand': 20846,\n",
       " 'subdivisions': 20847,\n",
       " \"veslefrikk's\": 20848,\n",
       " 'swollen': 11020,\n",
       " 'pulled': 7978,\n",
       " 'tilney': 20849,\n",
       " 'years': 203,\n",
       " 'structuring': 20850,\n",
       " 'episodes': 20851,\n",
       " 'sportscene': 16343,\n",
       " \"northair's\": 16344,\n",
       " 'jig': 20852,\n",
       " 'jin': 20853,\n",
       " 'jim': 3403,\n",
       " 'troubles': 8359,\n",
       " 'workforces': 13852,\n",
       " 'suspension': 2362,\n",
       " 'troubled': 3892,\n",
       " 'fondiaria': 16345,\n",
       " 'modestly': 6697,\n",
       " 'recipients': 12119,\n",
       " 'civilian': 7979,\n",
       " 'indigenous': 13853,\n",
       " 'overpowering': 20854,\n",
       " 'drilling': 1051,\n",
       " 'sorted': 16346,\n",
       " 'lichtenstein': 16347,\n",
       " 'bedevil': 20855,\n",
       " 'dispite': 20856,\n",
       " 'battleships': 16843,\n",
       " 'instability': 4824,\n",
       " 'quarter': 95,\n",
       " 'salado': 20857,\n",
       " 'honduras': 5692,\n",
       " \"chevron's\": 13855,\n",
       " \"lazere's\": 12273,\n",
       " 'receipt': 2660,\n",
       " 'sponsor': 8360,\n",
       " 'entering': 4825,\n",
       " \"kcbt's\": 16349,\n",
       " 'nowicki': 19987,\n",
       " 'salads': 13856,\n",
       " 'augar': 16351,\n",
       " '797': 7980,\n",
       " '796': 7254,\n",
       " '795': 8361,\n",
       " '794': 5295,\n",
       " '793': 5118,\n",
       " '792': 6170,\n",
       " '791': 5296,\n",
       " '790': 4826,\n",
       " \"nikko's\": 20858,\n",
       " 'unsaleable': 20859,\n",
       " '799': 5720,\n",
       " '798': 5693,\n",
       " 'seriously': 2143,\n",
       " 'trauma': 16352,\n",
       " 'tvbh': 20860,\n",
       " 'macedon': 20861,\n",
       " 'disintegrated': 21906,\n",
       " 'adddition': 21909,\n",
       " 'incentives': 2244,\n",
       " 'complicated': 5944,\n",
       " 'reevaluating': 20864,\n",
       " 'thatching': 21921,\n",
       " 'brasil': 7981,\n",
       " '79p': 20865,\n",
       " 'wrong': 4951,\n",
       " 'initiate': 8822,\n",
       " 'aboard': 16353,\n",
       " 'saving': 7255,\n",
       " 'spoken': 8823,\n",
       " 'parkinson': 16364,\n",
       " 'one': 65,\n",
       " 'ont': 20867,\n",
       " 'concert': 7256,\n",
       " \"boston's\": 16354,\n",
       " 'stifled': 13859,\n",
       " 'types': 4622,\n",
       " 'lingering': 20868,\n",
       " 'surges': 16356,\n",
       " 'hurdman': 20869,\n",
       " 'herds': 16357,\n",
       " 'absorbs': 14114,\n",
       " 'surged': 4681,\n",
       " 'dalkon': 14211,\n",
       " 'crossroads': 13860,\n",
       " 'shakeup': 20870,\n",
       " 'disasterous': 20871,\n",
       " 'illness': 11021,\n",
       " 'turned': 3242,\n",
       " 'locations': 3801,\n",
       " 'tyranite': 12120,\n",
       " 'minesweepers': 13861,\n",
       " 'turner': 7257,\n",
       " 'borough': 20872,\n",
       " 'underlines': 12358,\n",
       " \"bancorporation's\": 20873,\n",
       " 'fashionable': 20874,\n",
       " \"ae's\": 20875,\n",
       " 'dilutions': 16358,\n",
       " 'goodman': 9472,\n",
       " 'unlawfully': 10510,\n",
       " 'mayer': 16359,\n",
       " 'printer': 16360,\n",
       " 'offload': 20877,\n",
       " 'opposite': 13862,\n",
       " 'buffer': 738,\n",
       " 'printed': 9398,\n",
       " 'pequiven': 16361,\n",
       " 'panoche': 13863,\n",
       " 'knowingly': 20878,\n",
       " 'ecusta': 16362,\n",
       " 'thsl': 20879,\n",
       " 'phil': 8825,\n",
       " 'jitters': 13864,\n",
       " 'touche': 16363,\n",
       " 'jittery': 20881,\n",
       " 'friction': 3291,\n",
       " 'fecal': 16365,\n",
       " 'resurgance': 22068,\n",
       " 'heeding': 20882,\n",
       " 'soviets': 2363,\n",
       " 'imagined': 16366,\n",
       " 'transact': 16367,\n",
       " 'califoirnia': 20883,\n",
       " \"chrysler's\": 9399,\n",
       " 'respecitvely': 16368,\n",
       " 'presse': 16369,\n",
       " 'euromarket': 10084,\n",
       " 'guarded': 12121,\n",
       " 'satisfacotry': 16371,\n",
       " 'authroization': 20884,\n",
       " 'simplistic': 20885,\n",
       " 'monde': 20886,\n",
       " 'awaiting': 4102,\n",
       " 'recombinant': 13865,\n",
       " 'refinancement': 20887,\n",
       " 'comserv': 20888,\n",
       " 'kitakyushu': 20889,\n",
       " 'pima': 16372,\n",
       " 'basle': 11022,\n",
       " '6250': 20891,\n",
       " 'choudhury': 16373,\n",
       " 'vision': 8826,\n",
       " 'interruptible': 20892,\n",
       " 'weatherford': 13866,\n",
       " '832': 7982,\n",
       " '833': 5694,\n",
       " '830': 4420,\n",
       " '831': 5119,\n",
       " '836': 5297,\n",
       " '837': 4553,\n",
       " '834': 6172,\n",
       " '835': 4952,\n",
       " 'alarming': 22144,\n",
       " '838': 5695,\n",
       " '839': 6173,\n",
       " '524p': 20893,\n",
       " 'sponsorship': 20894,\n",
       " 'vendex': 12122,\n",
       " \"amsouth's\": 20895,\n",
       " 'kilometer': 20896,\n",
       " 'enjoys': 10086,\n",
       " 'illiberal': 20897,\n",
       " 'punta': 6174,\n",
       " 'punte': 20898,\n",
       " 'girozentrale': 10087,\n",
       " 'missstatements': 20899,\n",
       " 'marietta': 10088,\n",
       " 'awards': 6175,\n",
       " 'concentrated': 3635,\n",
       " '83p': 20900,\n",
       " 'developpement': 13867,\n",
       " 'rhodes': 13868,\n",
       " 'matheson': 5696,\n",
       " '1720': 20901,\n",
       " 'paring': 20902,\n",
       " 's': 35,\n",
       " 'concentrates': 4953,\n",
       " \"can's\": 16374,\n",
       " 'polysaturated': 22183,\n",
       " 'parini': 20903,\n",
       " 'baden': 13869,\n",
       " 'bader': 20904,\n",
       " 'buoyancy': 12123,\n",
       " 'erdem': 20905,\n",
       " 'properites': 16375,\n",
       " 'comparitive': 20906,\n",
       " 'practises': 12124,\n",
       " 'collides': 20907,\n",
       " 'west': 189,\n",
       " 'wess': 20908,\n",
       " 'collided': 13870,\n",
       " 'practised': 20909,\n",
       " \"amalgamated's\": 20910,\n",
       " 'motives': 20911,\n",
       " 'wants': 1378,\n",
       " 'formed': 1273,\n",
       " 'readings': 20912,\n",
       " 'geothermal': 12125,\n",
       " 'tightened': 7315,\n",
       " \"d'or\": 11023,\n",
       " 'former': 1109,\n",
       " 'venezulean': 20913,\n",
       " 'curd': 19935,\n",
       " 'squeezes': 12126,\n",
       " 'newspaper': 1019,\n",
       " 'situation': 817,\n",
       " 'ivey': 13871,\n",
       " 'engaged': 3636,\n",
       " 'dubious': 13872,\n",
       " 'cayacq': 17061,\n",
       " 'cobol': 20916,\n",
       " 'limping': 20917,\n",
       " 'technology': 883,\n",
       " 'koerner': 20919,\n",
       " 'debilitating': 16376,\n",
       " 'verified': 7983,\n",
       " 'otto': 4010,\n",
       " '7770': 20920,\n",
       " 'emulsions': 16377,\n",
       " \"onic's\": 16378,\n",
       " 'slate': 9075,\n",
       " 'wires': 20921,\n",
       " 'edged': 5506,\n",
       " 'assigns': 20922,\n",
       " 'singapore': 1341,\n",
       " 'deflate': 20923,\n",
       " \"strategy's\": 20924,\n",
       " 'walesa': 16379,\n",
       " 'advertisement': 4554,\n",
       " 'luyten': 20925,\n",
       " 'shrortly': 20926,\n",
       " 'corpoartion': 20927,\n",
       " 'preferance': 22290,\n",
       " 'tracking': 16380,\n",
       " 'sunnyvale': 13874,\n",
       " 'colorants': 20928,\n",
       " 'persistently': 16381,\n",
       " \"officers'\": 16382,\n",
       " \"his's\": 20929,\n",
       " 'being': 367,\n",
       " 'divestitures': 7259,\n",
       " 'steamer': 20930,\n",
       " 'rover': 20931,\n",
       " 'grounded': 8362,\n",
       " \"businessmen's\": 16383,\n",
       " 'cyanidation': 16384,\n",
       " 'overthrow': 20932,\n",
       " 'partnerhip': 20933,\n",
       " 'sumt': 16385,\n",
       " 'sums': 8827,\n",
       " 'oelmuehle': 16386,\n",
       " 'unveil': 16387,\n",
       " 'gestures': 13875,\n",
       " 'penta': 20934,\n",
       " 'traffic': 2544,\n",
       " 'preference': 2428,\n",
       " 'sumi': 20935,\n",
       " 'world': 166,\n",
       " 'postal': 9400,\n",
       " 'bced': 16388,\n",
       " 'dornbush': 12128,\n",
       " 'confine': 14215,\n",
       " '2555': 20936,\n",
       " \"zambia's\": 5945,\n",
       " 'superiority': 20937,\n",
       " 'militate': 20938,\n",
       " 'satisfactory': 2395,\n",
       " 'superintendent': 20939,\n",
       " 'tvx': 5946,\n",
       " 'tvt': 16389,\n",
       " 'magma': 6698,\n",
       " 'diving': 20940,\n",
       " 'tvb': 15548,\n",
       " 'seaman': 13876,\n",
       " 'matsunaga': 11025,\n",
       " '919': 4827,\n",
       " '918': 5298,\n",
       " 'refundable': 17070,\n",
       " '914': 5947,\n",
       " '917': 7260,\n",
       " '916': 6699,\n",
       " '911': 5507,\n",
       " '910': 4828,\n",
       " 'restoring': 10213,\n",
       " '912': 4555,\n",
       " 'squabble': 20942,\n",
       " 'retains': 7261,\n",
       " \"partner's\": 20943,\n",
       " 'leadership': 5300,\n",
       " 'graaf': 11026,\n",
       " 'spacelab': 20944,\n",
       " 'thailand': 1800,\n",
       " 'graan': 9402,\n",
       " 'exasperating': 20945,\n",
       " 'hartmarx': 12129,\n",
       " 'frights': 16390,\n",
       " 'niall': 20946,\n",
       " 'johnston': 11027,\n",
       " '91p': 16391,\n",
       " 'sensitively': 16392,\n",
       " 'porsche': 6016,\n",
       " 'prepares': 15494,\n",
       " 'lively': 12130,\n",
       " 'stoppages': 10686,\n",
       " \"associated's\": 16394,\n",
       " 'pivot': 12131,\n",
       " 'series': 1037,\n",
       " 'sese': 24050,\n",
       " 'bubble': 7604,\n",
       " 'trusses': 16395,\n",
       " 'interestate': 20949,\n",
       " 'continents': 20950,\n",
       " 'societal': 20951,\n",
       " 'with': 28,\n",
       " 'pull': 6176,\n",
       " 'rush': 6700,\n",
       " 'monopoly': 6222,\n",
       " 'operationally': 20953,\n",
       " 'dirty': 20954,\n",
       " 'abuses': 10090,\n",
       " 'prudhoe': 7262,\n",
       " 'pulp': 5949,\n",
       " 'rust': 16396,\n",
       " 'hellman': 20955,\n",
       " 'amdec': 20956,\n",
       " 'australasian': 16397,\n",
       " 'watches': 13878,\n",
       " 'hypertension': 20957,\n",
       " \"hemdale's\": 20958,\n",
       " 'formulation': 16398,\n",
       " 'watched': 7605,\n",
       " 'jargon': 20959,\n",
       " 'cream': 13879,\n",
       " 'ideally': 9404,\n",
       " 'ryavec': 11028,\n",
       " 'microoganisms': 20960,\n",
       " 'indemnify': 13880,\n",
       " 'wincenty': 20961,\n",
       " 'waving': 20962,\n",
       " \"multifood's\": 20963,\n",
       " 'midges': 20964,\n",
       " 'natalie': 11029,\n",
       " 'crosbie': 13881,\n",
       " 'posible': 20965,\n",
       " 'omnibus': 13882,\n",
       " 'assetsof': 20966,\n",
       " 'tricks': 13883,\n",
       " 'rs': 16399,\n",
       " 'kilogram': 20967,\n",
       " 'pruning': 25363,\n",
       " 'dyer': 13884,\n",
       " 'dyes': 20968,\n",
       " 'legislatures': 20969,\n",
       " 'scm': 16400,\n",
       " 'sci': 9405,\n",
       " 'riedel': 20970,\n",
       " 'ceramic': 16401,\n",
       " 'unitholders': 6701,\n",
       " 'scb': 13885,\n",
       " 'dn11': 20971,\n",
       " 'conditionality': 20972,\n",
       " \"stock's\": 13807,\n",
       " 'masland': 20973,\n",
       " 'causes': 7606,\n",
       " 'riots': 10091,\n",
       " 'norf': 20974,\n",
       " 'nord': 9406,\n",
       " 'midwest': 3893,\n",
       " 'tamils': 13886,\n",
       " 'ofthe': 16402,\n",
       " \"colombia's\": 3421,\n",
       " '24th': 11030,\n",
       " 'sant': 20975,\n",
       " 'moines': 10092,\n",
       " 'electrotechnical': 22577,\n",
       " 'proceeded': 24534,\n",
       " 'sanz': 20976,\n",
       " 'insufficiently': 13887,\n",
       " 'sang': 20977,\n",
       " 'sand': 5950,\n",
       " 'bracho': 16404,\n",
       " 'small': 805,\n",
       " 'workloads': 20978,\n",
       " 'sank': 6702,\n",
       " 'kemper': 20979,\n",
       " 'abbreviated': 16405,\n",
       " 'quicker': 13888,\n",
       " '199': 3802,\n",
       " '198': 3243,\n",
       " '195': 2661,\n",
       " '194': 3080,\n",
       " '197': 4310,\n",
       " '196': 3894,\n",
       " '191': 2850,\n",
       " '190': 2199,\n",
       " '193': 3481,\n",
       " '192': 3350,\n",
       " 'past': 582,\n",
       " 'fractionation': 20980,\n",
       " 'displays': 20981,\n",
       " 'pass': 3081,\n",
       " 'investment': 202,\n",
       " 'quals': 27062,\n",
       " 'quicken': 16406,\n",
       " \"centronic's\": 20983,\n",
       " 'menswear': 20984,\n",
       " 'clock': 16407,\n",
       " 'teape': 20985,\n",
       " 'teapa': 20986,\n",
       " 'prevailed': 10093,\n",
       " 'hebei': 9407,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# we can reverse a newswire back to words\r\n",
    "word_index = reuters.get_word_index()\r\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\r\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\r\n",
    "train_data[0]])\r\n",
    "#label associated with an example is an integer between 0 and 45 (topic index)\r\n",
    "print(train_labels[10])\r\n",
    "#the decoded newswire\r\n",
    "print(decoded_newswire)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# METHODOLOGY\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PREPARE DATA\n",
    "\n",
    "We start with data preprocessing and we also encode the labels as one-hot categorically encoded. One-hot encoding is a widely used format for categorical data.\n",
    "\n",
    "Some considerations:\n",
    "- Format as tensors usually scaled to small values range [0,1] or ][-1,1]\n",
    "- If data is heterogeneous (different ranges) it should be normalized\n",
    "- Do some feature engineering for small data problem\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "import numpy as np\r\n",
    "def vectorize_sequences(sequences, dimension=10000):\r\n",
    "    results = np.zeros((len(sequences), dimension))\r\n",
    "    for i, sequence in enumerate(sequences):\r\n",
    "        results[i, sequence] = 1.\r\n",
    "    return results\r\n",
    "x_train = vectorize_sequences(train_data)\r\n",
    "x_test = vectorize_sequences(test_data)\r\n",
    "\r\n",
    "# vectorize training data\r\n",
    "y_train = np.asarray(train_labels).astype('float32')\r\n",
    "# vectorize testing data\r\n",
    "y_test = np.asarray(train_labels).astype('float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# vectorize labels - one-hot encoding embeds each label as an all-zero vector with 1 in the place of the label index\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "#vectorize training labels\r\n",
    "one_hot_train_labels = to_categorical(train_labels)\r\n",
    "#vectorize test labels\r\n",
    "one_hot_test_labels = to_categorical(test_labels)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODEL\n",
    "\n",
    "We are trying and classify shorts snippets of text. What we want to achieve is statistical power.\n",
    "\n",
    "3 choices to build our working model:\n",
    "\n",
    "- Last Layer Activation (Constraints on the network output)\n",
    "- Loss Function (match the type of problem we are solving (Cross-Entropy, MSE, ...)\n",
    "- Optimization Configuration (default is RMSPROP)\n",
    "\n",
    "    \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "from keras import models\r\n",
    "from keras import layers\r\n",
    "\r\n",
    "model = models.Sequential()\r\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,)))\r\n",
    "model.add(layers.Dense(64, activation = 'relu'))\r\n",
    "#last layer use a softmax activation to output a probability distribution over the 46 different classes\r\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best loss function is categorical_crossentropy since it measures the distance between 2 probability distributions (the output one and the true distributions of the labels).\n",
    "\n",
    "We want to minimize this distance to get an output as close as possible to true labels.\n",
    "\n",
    "We want also to understand which metrics to use.\n",
    "\n",
    "Being a balanced classification we will use accuracy (ROC AUC could be another option)\n",
    "\n",
    "If it was an inbalanced classification we could use precision and recall.\n",
    "\n",
    "Other metrics commonly used are mean,average,precision.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "# weight and bias tensors\r\n",
    "def print_layer_tensor_shape(layer):\r\n",
    "    weight_params = model.layers[layer].get_weights()[0]\r\n",
    "    bias_params = model.layers[layer].get_weights()[1]\r\n",
    "    print(layer, '\\t', weight_params.shape, '\\t', bias_params.shape)\r\n",
    "    \r\n",
    "print_layer_tensor_shape(layer = 0)\r\n",
    "print_layer_tensor_shape(layer = 1)\r\n",
    "print_layer_tensor_shape(layer = 2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 \t (10000, 64) \t (64,)\n",
      "1 \t (64, 64) \t (64,)\n",
      "2 \t (64, 46) \t (46,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EVALUATION PROTOCOL\n",
    "\n",
    "Considerations:\n",
    "\n",
    "Maintain a hold-out validation set\n",
    "\n",
    "Doing K-fold cross validation\n",
    "\n",
    "Doing iterated K-fold validation\n",
    "\n",
    "    \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now want to validate our approach and to do this we will create a validation set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "# we slice 1000 sample in the training data to be reserved as validation set\r\n",
    "x_val = x_train[:1000]\r\n",
    "partial_x_train = x_train[1000:]\r\n",
    "\r\n",
    "y_val = one_hot_train_labels[:1000]\r\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "# we train the model\r\n",
    "history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 2.5371 - accuracy: 0.5436 - val_loss: 1.6939 - val_accuracy: 0.6430\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.4053 - accuracy: 0.7082 - val_loss: 1.3064 - val_accuracy: 0.7160\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.0546 - accuracy: 0.7780 - val_loss: 1.1516 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8352 - accuracy: 0.8252 - val_loss: 1.0490 - val_accuracy: 0.7850\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.6657 - accuracy: 0.8616 - val_loss: 0.9671 - val_accuracy: 0.7990\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5355 - accuracy: 0.8909 - val_loss: 0.9191 - val_accuracy: 0.8090\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.4350 - accuracy: 0.9100 - val_loss: 0.8904 - val_accuracy: 0.8140\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.3491 - accuracy: 0.9277 - val_loss: 0.9004 - val_accuracy: 0.8120\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.2930 - accuracy: 0.9355 - val_loss: 0.8818 - val_accuracy: 0.8150\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.2443 - accuracy: 0.9430 - val_loss: 0.8879 - val_accuracy: 0.8160\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.2150 - accuracy: 0.9485 - val_loss: 0.9063 - val_accuracy: 0.8120\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1842 - accuracy: 0.9514 - val_loss: 0.9561 - val_accuracy: 0.8100\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1684 - accuracy: 0.9529 - val_loss: 0.9141 - val_accuracy: 0.8150\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1535 - accuracy: 0.9555 - val_loss: 0.9522 - val_accuracy: 0.8070\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1407 - accuracy: 0.9565 - val_loss: 0.9500 - val_accuracy: 0.8100\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1336 - accuracy: 0.9544 - val_loss: 1.0103 - val_accuracy: 0.8040\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1280 - accuracy: 0.9564 - val_loss: 0.9863 - val_accuracy: 0.8090\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1202 - accuracy: 0.9578 - val_loss: 1.0072 - val_accuracy: 0.8080\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1163 - accuracy: 0.9577 - val_loss: 1.0465 - val_accuracy: 0.8010\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1127 - accuracy: 0.9584 - val_loss: 1.0997 - val_accuracy: 0.7870\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 647,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "history.history.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "#we plot the training and validation loss\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "loss = history.history['loss']\r\n",
    "val_loss = history.history['val_loss']\r\n",
    "\r\n",
    "epochs = range(1,len(loss) + 1)\r\n",
    "\r\n",
    "plt.plot(epochs,loss,'bo',label = 'Training Loss')\r\n",
    "plt.plot(epochs,val_loss,'b',label='Validation Loss')\r\n",
    "plt.title('Training and Validation Loss')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Loss')\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# we plot the training and validation accuracy\r\n",
    "accuracy = history.history['accuracy']\r\n",
    "val_accuracy = history.history['val_accuracy']\r\n",
    "\r\n",
    "plt.plot(epochs,accuracy,'bo',label='Training Accuracy')\r\n",
    "plt.plot(epochs,val_accuracy,'b',label='Validation Accuracy')\r\n",
    "plt.title('Training and Validation Accuracy')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Loss')\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "print(np.argmin(val_loss), 'epochs')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8 epochs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RESULTS\n",
    "\n",
    "We want to develop a model that neither underfits or overfits.\n",
    "The challenge is between optimization and generalization. \n",
    "Optimization refers to the process of adjusting a model to get the best performance possible on the training data.\n",
    "Generalization refers to how well the trained model would perform on data it has never seen before. \n",
    "\n",
    "The processing of fighting overfitting is called regularization.\n",
    "\n",
    "To develop a model that neither underfits or overfits we can:\n",
    "- add layers\n",
    "- make the layers bigger\n",
    "- train for more epochs\n",
    "\n",
    "\n",
    "For regularization and tuning we can:\n",
    "- add dropout\n",
    "- try different architectures: add or remove layers\n",
    "- add L1 and/or L2 regularization\n",
    "- try different hyperparameters to find optimal configuration\n",
    "- iterate on feature engineering, add or remove features\n",
    "- reduce the network size\n",
    "\n",
    "There is not single recipe to determine the the right number of layers or their size, evaluation of different architectures is important in order to find the right balance for your data. You can star small and increase gradually until you get diminishing returns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## REDUCE NETWORK SIZE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "We can notice from the plot that our network begin to overfit after 8 epochs.\n",
    "\n",
    "We can test creating a new network for 8 epochs and evaluate it again on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "model = models.Sequential()\r\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,)))\r\n",
    "model.add(layers.Dense(64, activation = 'relu'))\r\n",
    "#last layer use a softmax activation to output a probability distribution over the 46 different classes\r\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\r\n",
    "\r\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
    "\r\n",
    "model.fit(partial_x_train,partial_y_train,epochs=8,batch_size=512,validation_data=(x_val,y_val))\r\n",
    "\r\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\r\n",
    "\r\n",
    "print(results)\r\n",
    "\r\n",
    "model.summary()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/8\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 2.7111 - accuracy: 0.4707 - val_loss: 1.8048 - val_accuracy: 0.6410\n",
      "Epoch 2/8\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 1.4423 - accuracy: 0.7073 - val_loss: 1.3153 - val_accuracy: 0.7200\n",
      "Epoch 3/8\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 1.0445 - accuracy: 0.7771 - val_loss: 1.1247 - val_accuracy: 0.7650\n",
      "Epoch 4/8\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.8188 - accuracy: 0.8290 - val_loss: 1.0225 - val_accuracy: 0.7910\n",
      "Epoch 5/8\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.6488 - accuracy: 0.8659 - val_loss: 0.9501 - val_accuracy: 0.8050\n",
      "Epoch 6/8\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.5182 - accuracy: 0.8924 - val_loss: 0.9299 - val_accuracy: 0.8010\n",
      "Epoch 7/8\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.4188 - accuracy: 0.9132 - val_loss: 0.9550 - val_accuracy: 0.7950\n",
      "Epoch 8/8\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.3413 - accuracy: 0.9278 - val_loss: 0.8936 - val_accuracy: 0.8070\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9757 - accuracy: 0.7854\n",
      "[0.9757221341133118, 0.7853962779045105]\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 647,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get an accuracy of approximately 79% which is quite good."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GENERATE PREDICTIONS ON NEW DATA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can generate topic predictions on the test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "# outline the predictions success\r\n",
    "predictions = model.predict(x_test)\r\n",
    "print('wire', '\\t', 'prod', '\\t', 'label')\r\n",
    "for i in range(46):\r\n",
    "    print(i, '\\t', np.argmax(predictions[i]), '\\t', test_labels[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "wire \t prod \t label\n",
      "0 \t 3 \t 3\n",
      "1 \t 10 \t 10\n",
      "2 \t 1 \t 1\n",
      "3 \t 4 \t 4\n",
      "4 \t 13 \t 4\n",
      "5 \t 3 \t 3\n",
      "6 \t 3 \t 3\n",
      "7 \t 3 \t 3\n",
      "8 \t 3 \t 3\n",
      "9 \t 3 \t 3\n",
      "10 \t 1 \t 5\n",
      "11 \t 4 \t 4\n",
      "12 \t 1 \t 1\n",
      "13 \t 3 \t 3\n",
      "14 \t 1 \t 1\n",
      "15 \t 11 \t 11\n",
      "16 \t 4 \t 23\n",
      "17 \t 3 \t 3\n",
      "18 \t 19 \t 19\n",
      "19 \t 3 \t 3\n",
      "20 \t 8 \t 8\n",
      "21 \t 3 \t 3\n",
      "22 \t 3 \t 3\n",
      "23 \t 4 \t 3\n",
      "24 \t 9 \t 9\n",
      "25 \t 3 \t 3\n",
      "26 \t 4 \t 4\n",
      "27 \t 6 \t 6\n",
      "28 \t 10 \t 10\n",
      "29 \t 3 \t 3\n",
      "30 \t 3 \t 3\n",
      "31 \t 10 \t 10\n",
      "32 \t 4 \t 20\n",
      "33 \t 3 \t 1\n",
      "34 \t 19 \t 19\n",
      "35 \t 4 \t 4\n",
      "36 \t 19 \t 40\n",
      "37 \t 1 \t 1\n",
      "38 \t 4 \t 4\n",
      "39 \t 3 \t 3\n",
      "40 \t 3 \t 15\n",
      "41 \t 21 \t 21\n",
      "42 \t 3 \t 3\n",
      "43 \t 34 \t 34\n",
      "44 \t 4 \t 4\n",
      "45 \t 4 \t 4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "predictions = model.predict(x_test)\r\n",
    "predictions[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "each entry is a vector of lenght 46"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "np.sum(predictions[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the coefficients sum is 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "np.argmax(predictions[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "this is the class with the largest entry and the class with highest probability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LARGE INTERMEDIATE LAYERS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the outputs have 46 dimensions we should avoid intermediate layers with values much smaller than 46.\n",
    "\n",
    "If we introduce a small intermediate layer we risk creating an information bottleneck."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "model = models.Sequential()\r\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,)))\r\n",
    "# we introduce a small intermediate layer\r\n",
    "model.add(layers.Dense(4, activation = 'relu'))\r\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\r\n",
    "\r\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
    "\r\n",
    "model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=128,validation_data=(x_val,y_val))\r\n",
    "\r\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\r\n",
    "\r\n",
    "print(results)\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 3.3697 - accuracy: 0.2175 - val_loss: 2.9096 - val_accuracy: 0.2270\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.4485 - accuracy: 0.4035 - val_loss: 2.0027 - val_accuracy: 0.5800\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.6926 - accuracy: 0.5960 - val_loss: 1.6476 - val_accuracy: 0.5900\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.4613 - accuracy: 0.5980 - val_loss: 1.5544 - val_accuracy: 0.5910\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.3110 - accuracy: 0.6393 - val_loss: 1.4973 - val_accuracy: 0.6360\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.1916 - accuracy: 0.6893 - val_loss: 1.4574 - val_accuracy: 0.6480\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.0949 - accuracy: 0.7125 - val_loss: 1.4613 - val_accuracy: 0.6590\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.0185 - accuracy: 0.7318 - val_loss: 1.4546 - val_accuracy: 0.6690\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.9552 - accuracy: 0.7503 - val_loss: 1.4874 - val_accuracy: 0.6730\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.9057 - accuracy: 0.7666 - val_loss: 1.4979 - val_accuracy: 0.6750\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.8609 - accuracy: 0.7815 - val_loss: 1.5473 - val_accuracy: 0.6750\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.8206 - accuracy: 0.7909 - val_loss: 1.5886 - val_accuracy: 0.6850\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.7852 - accuracy: 0.7980 - val_loss: 1.6412 - val_accuracy: 0.6800\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.7542 - accuracy: 0.8024 - val_loss: 1.7016 - val_accuracy: 0.6720\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.7273 - accuracy: 0.8038 - val_loss: 1.7532 - val_accuracy: 0.6690\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.7021 - accuracy: 0.8052 - val_loss: 1.8182 - val_accuracy: 0.6770\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6796 - accuracy: 0.8102 - val_loss: 1.8603 - val_accuracy: 0.6670\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6599 - accuracy: 0.8135 - val_loss: 1.9360 - val_accuracy: 0.6660\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.6438 - accuracy: 0.8162 - val_loss: 2.0426 - val_accuracy: 0.6620\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.6286 - accuracy: 0.8172 - val_loss: 2.0238 - val_accuracy: 0.6660\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 2.2216 - accuracy: 0.6536\n",
      "[2.221562385559082, 0.6536064147949219]\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 46)                230       \n",
      "=================================================================\n",
      "Total params: 640,554\n",
      "Trainable params: 640,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy drops to approximately 70% since we are trying and compress a lot of information in a small bucket of 4."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also experiment with larger and smaller layers and different combinations to see how our accuracy change."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "model = models.Sequential()\r\n",
    "model.add(layers.Dense(128, activation = 'relu', input_shape = (10000,)))\r\n",
    "model.add(layers.Dense(128, activation = 'relu'))\r\n",
    "#last layer use a softmax activation to output a probability distribution over the 46 different classes\r\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\r\n",
    "\r\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
    "\r\n",
    "model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))\r\n",
    "\r\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\r\n",
    "\r\n",
    "print(results)\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 2.2808 - accuracy: 0.5666 - val_loss: 1.4072 - val_accuracy: 0.6750\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 1.1290 - accuracy: 0.7582 - val_loss: 1.1175 - val_accuracy: 0.7530\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.7943 - accuracy: 0.8327 - val_loss: 0.9810 - val_accuracy: 0.7890\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5607 - accuracy: 0.8825 - val_loss: 0.9092 - val_accuracy: 0.8050\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.4136 - accuracy: 0.9162 - val_loss: 0.9006 - val_accuracy: 0.8010\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.3186 - accuracy: 0.9302 - val_loss: 0.8615 - val_accuracy: 0.8250\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.2548 - accuracy: 0.9449 - val_loss: 0.9130 - val_accuracy: 0.8130\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.2066 - accuracy: 0.9481 - val_loss: 0.9249 - val_accuracy: 0.8100\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.1829 - accuracy: 0.9523 - val_loss: 1.0064 - val_accuracy: 0.7970\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.1629 - accuracy: 0.9536 - val_loss: 0.9289 - val_accuracy: 0.8180\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.1482 - accuracy: 0.9538 - val_loss: 0.9761 - val_accuracy: 0.8010\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.1340 - accuracy: 0.9560 - val_loss: 0.9998 - val_accuracy: 0.8170\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.1389 - accuracy: 0.9541 - val_loss: 1.0711 - val_accuracy: 0.8030\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.1309 - accuracy: 0.9568 - val_loss: 0.9946 - val_accuracy: 0.8140\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.1220 - accuracy: 0.9572 - val_loss: 1.0784 - val_accuracy: 0.8080\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.1213 - accuracy: 0.9570 - val_loss: 1.0873 - val_accuracy: 0.8030\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.1163 - accuracy: 0.9567 - val_loss: 1.0438 - val_accuracy: 0.8060\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.1183 - accuracy: 0.9563 - val_loss: 1.1507 - val_accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.1116 - accuracy: 0.9563 - val_loss: 1.0083 - val_accuracy: 0.8120\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.1039 - accuracy: 0.9569 - val_loss: 1.1924 - val_accuracy: 0.7890\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.3913 - accuracy: 0.7698\n",
      "[1.391297459602356, 0.7698130011558533]\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 128)               1280128   \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 1,302,574\n",
      "Trainable params: 1,302,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "#last layer use a softmax activation to output a probability distribution over the 46 different classes\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val,y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(results)\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 26ms/step - loss: 2.9048 - accuracy: 0.4352 - val_loss: 2.1941 - val_accuracy: 0.5590\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.8573 - accuracy: 0.6155 - val_loss: 1.6433 - val_accuracy: 0.6550\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.4096 - accuracy: 0.7090 - val_loss: 1.3904 - val_accuracy: 0.7040\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 1.1547 - accuracy: 0.7509 - val_loss: 1.2608 - val_accuracy: 0.7180\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9786 - accuracy: 0.7875 - val_loss: 1.1507 - val_accuracy: 0.7530\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8382 - accuracy: 0.8196 - val_loss: 1.0888 - val_accuracy: 0.7730\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7206 - accuracy: 0.8423 - val_loss: 1.0298 - val_accuracy: 0.7920\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6208 - accuracy: 0.8657 - val_loss: 1.0024 - val_accuracy: 0.8030\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5357 - accuracy: 0.8855 - val_loss: 0.9705 - val_accuracy: 0.8020\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.4620 - accuracy: 0.9003 - val_loss: 0.9598 - val_accuracy: 0.7980\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.4000 - accuracy: 0.9129 - val_loss: 0.9620 - val_accuracy: 0.8050\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3478 - accuracy: 0.9278 - val_loss: 0.9407 - val_accuracy: 0.8050\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.3029 - accuracy: 0.9345 - val_loss: 0.9599 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2659 - accuracy: 0.9414 - val_loss: 0.9784 - val_accuracy: 0.7990\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.2363 - accuracy: 0.9451 - val_loss: 0.9535 - val_accuracy: 0.8140\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.2094 - accuracy: 0.9480 - val_loss: 0.9629 - val_accuracy: 0.8160\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1890 - accuracy: 0.9519 - val_loss: 1.0042 - val_accuracy: 0.7990\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1719 - accuracy: 0.9521 - val_loss: 1.0283 - val_accuracy: 0.7960\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.1614 - accuracy: 0.9545 - val_loss: 1.0243 - val_accuracy: 0.8010\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.1501 - accuracy: 0.9555 - val_loss: 1.0211 - val_accuracy: 0.8080\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1187 - accuracy: 0.7823\n",
      "[1.1186721324920654, 0.7822796106338501]\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_105 (Dense)            (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 46)                1518      \n",
      "=================================================================\n",
      "Total params: 322,606\n",
      "Trainable params: 322,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CONCLUSIONS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to scale our model and develop one that neither underfit or overfit.\n",
    "We can reach this objective in many ways by adding layers or increasing the size of layers or train for more epochs.\n",
    "\n",
    "If we are classifying data points among N classes we want to use a dense final layer of size N (46 in our case).\n",
    "We used in our final layer a softmax activation in order to output a probability distribution over the 46 classes.\n",
    "\n",
    "Categorical Crossentropy is the standard for such problems since it minimize the distance between the probability distributions (the output one and the true distributions of the labels).\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}